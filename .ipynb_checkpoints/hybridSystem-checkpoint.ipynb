{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#movie rec using collaborative data and matrix factorization\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from scipy import optimize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#some code from http://alexabate.github.io/2016/11/05/movie-lens.html\n",
    "#some code from https://nikhilwins.wordpress.com/2015/09/18/movie-recommendations-how-does-netflix-do-it-a-9-step-coding-intuitive-guide-into-collaborative-filtering/\n",
    "\n",
    "user_item_ratings = pd.read_csv('ratings.csv', sep=',')\n",
    "#user_item_ratings.head()\n",
    "number_of_unique_users = len(user_item_ratings['userId'].unique())\n",
    "number_of_unique_movies = len(user_item_ratings['movieId'].unique())\n",
    "number_of_ratings = len(user_item_ratings)\n",
    "#print user_item_ratings\n",
    "print number_of_unique_users\n",
    "print number_of_unique_movies\n",
    "\n",
    "ratingsMatrix=pd.pivot_table(user_item_ratings, values='rating', fill_value=0, columns='userId', index='movieId')#\n",
    "#print ratingsMatrix.head\n",
    "\n",
    "ratingsMatrix_test=ratingsMatrix.iloc[6044:,:]\n",
    "\n",
    "ratingsMatrix=ratingsMatrix.iloc[:6043,:]\n",
    "\n",
    "\n",
    "did_rate = (ratingsMatrix != 0) * 1\n",
    "did_rate_test=(ratingsMatrix_test != 0) * 1\n",
    "\n",
    "#n_movies=number_of_unique_movies\n",
    "n_movies=ratingsMatrix.shape[0]\n",
    "n_users=number_of_unique_users\n",
    "\n",
    "\n",
    "#normalize_ratings from https://nikhilwins.wordpress.com/2015/09/18/movie-recommendations-how-does-netflix-do-it-a-9-step-coding-intuitive-guide-into-collaborative-filtering/\n",
    "def normalize_ratings(ratings, did_rate):\n",
    "    num_movies = ratings.shape[0]\n",
    "    ratings_mean = np.zeros(shape = (num_movies, 1))\n",
    "    ratings_norm = np.zeros(shape = ratings.shape)\n",
    "    \n",
    "    for i in range(0, num_movies):\n",
    "        # Get all the indexes where there is a 1\n",
    "        idx = np.where(did_rate.iloc[i,:] ==1)[0]\n",
    "        # Calculate mean rating of ith movie only from users that gave a rating\n",
    "        ratings_mean[i] = np.mean(ratings.iloc[i, idx])\n",
    "         #ratings_mean[i]\n",
    "        ratings_norm[i, idx] = ratings.iloc[i, idx] - ratings_mean[i]\n",
    "        #(EKC: make sure these indices aren't off by one)\n",
    "        #print i\n",
    "    return (ratings_norm, ratings_mean)\n",
    "\n",
    "ratings_norm, ratings_mean = normalize_ratings(ratingsMatrix, did_rate)\n",
    "\n",
    "did_rate=did_rate.as_matrix()\n",
    "\n",
    "svd = TruncatedSVD(n_components=3, n_iter=10, random_state=42)\n",
    "X_tr = svd.fit_transform(ratings_norm)\n",
    "print X_tr.shape\n",
    "\n",
    "X_o = svd.inverse_transform(X_tr)\n",
    "#that would be the predicted ratings\n",
    "difference = X_o*did_rate-ratings_norm\n",
    "cost = np.sum((difference)**2)/2\n",
    "print cost\n",
    "\n",
    "movie_info = pd.read_csv('movies.csv', sep=',')\n",
    "\n",
    "\n",
    "\n",
    "#need a list of all the genres--basically just go through the rows and add whichever ones are unique\n",
    "all_genres=set()\n",
    "for row in range(0,movie_info.shape[0]):\n",
    "    genres=movie_info['genres'][row]\n",
    "    genres_split=genres.split('|')\n",
    "    all_genres.update(genres_split)\n",
    "print all_genres\n",
    "all_genres=list(all_genres)\n",
    "print all_genres[0]\n",
    "    \n",
    "n_features=len(all_genres)\n",
    " \n",
    "n_movies_genres=len(movie_info['movieId'].unique())\n",
    "genres_in_descriptions=pd.DataFrame(np.zeros([n_movies_genres, n_features]))\n",
    "print genres_in_descriptions.shape\n",
    "#just want to combine the movies\n",
    "print set(movie_info['movieId'].unique()) - set(user_item_ratings['movieId'].unique())\n",
    "print set(user_item_ratings['movieId'].unique()) - set(movie_info['movieId'].unique())\n",
    "#some movies have genres but haven't been rated. ignoring them for now.\n",
    "# DELETE UNRATED MOVIES\n",
    "#this is slow but ony has to happen once\n",
    "n_movies=len(user_item_ratings['movieId'].unique())\n",
    "genresMatrix=np.zeros([n_movies, n_features])\n",
    "#print ratings_norm.shape\n",
    "for ii in range(0,n_movies):\n",
    "    thisMovie=ratingsMatrix.index.values[ii]\n",
    "    Index=np.where(movie_info['movieId']==thisMovie)\n",
    "    genres=movie_info['genres'][Index[0]]\n",
    "    #genres_split=genres.split('|')\n",
    "    gInd=0\n",
    "    for g in all_genres:\n",
    "        if g in str(genres):\n",
    "            genresMatrix[ii, gInd]=1\n",
    "        gInd=gInd+1\n",
    "    \n",
    "    \n",
    "    #convert ratings to probability of liking movie\n",
    "# first z-score them (express each rating in terms of the standard deviation of that user's ratings)\n",
    "stddevs=np.std(ratings_norm,axis=0)\n",
    "ratings_zscored=ratings_norm/(stddevs)\n",
    "# then get the probability (note that this assumes the ratings are distributed normally)\n",
    "from scipy.stats import norm\n",
    "probs=norm.cdf(ratings_zscored)\n",
    "\n",
    "#bayesian model\n",
    "\n",
    "ProbabilityOfLikingMovies=probs[1:,:].T # \n",
    "wordsInAllDescriptions=genresMatrix[1:,:]\n",
    "#(leave one out for testing--later decide on the best cross-validation method)\n",
    "\n",
    "#log likelihood of a user liking a movie is the sum of P(like) for all movies containing each word in the movie's description\n",
    "# movie's description=genre (and tags? but user would not have tagged a new movie. Can include all tags for each movie?)\n",
    "weightedP=ProbabilityOfLikingMovies.dot(wordsInAllDescriptions) #\n",
    "                #(1-by-movies)         *  (movies-by-featurewords)\n",
    "                #(users-by-movies)         *  (movies-by-featurewords)\n",
    "            \n",
    "wordsInMovieDescription=genresMatrix[0,:]\n",
    "    \n",
    "logPLikeMovie=weightedP.dot(wordsInMovieDescription)\n",
    "#(users-by-features * features-by-1?)\n",
    "#sum the probability of liking each movie that contains each word--this is a matrix multiplication, right?\n",
    "\n",
    "\n",
    "# do this for every movie\n",
    "logPAllMovies=np.zeros(ratingsMatrix.T.shape)\n",
    "for ii in range(0,n_movies):\n",
    "    ProbabilityOfLikingMovies=np.delete(probs, ii, 0).T # \n",
    "    wordsInAllDescriptions=np.delete(genresMatrix, ii, 0)\n",
    "    #(leave one out for testing--later decide on the best cross-validation method)\n",
    "\n",
    "    #log likelihood of a user liking a movie is the sum of P(like) for all movies containing each word in the movie's description\n",
    "    # movie's description=genre (and tags? but user would not have tagged a new movie. Can include all tags for each movie?)\n",
    "    weightedP=ProbabilityOfLikingMovies.dot(wordsInAllDescriptions) #\n",
    "                    #(1-by-movies)         *  (movies-by-featurewords)\n",
    "                    #(users-by-movies)         *  (movies-by-featurewords)\n",
    "            \n",
    "    wordsInMovieDescription=genresMatrix[ii,:]\n",
    "    \n",
    "    logPLikeMovie=weightedP.dot(wordsInMovieDescription)\n",
    "    #(users-by-features * features-by-1?)\n",
    "    \n",
    "    logPAllMovies[:,ii]=logPLikeMovie\n",
    "    \n",
    "    #sum the probability of liking each movie that contains each word--this is a matrix multiplication, right?\n",
    "    if ii % 50 == 0:\n",
    "        print ii\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#convert the log likelihoods to z scores and then to ratings--fill in the ratings matrix \n",
    "predictedRatingsNorm=logPAllMovies.T\n",
    "PRNmeans=np.mean(predictedRatingsNorm, axis=0)\n",
    "PRNstddevs=np.std(predictedRatingsNorm,axis=0)\n",
    "\n",
    "predictedRatingsNorm=(predictedRatingsNorm-PRNmeans)/(PRNstddevs)\n",
    "#scale by the standard rating deviation\n",
    "predictedRatingsNorm=predictedRatingsNorm * stddevs #(make sure that's the right way to actually do this multiplication)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pedictedRatingsNorm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f7af6faf9431>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# fill in missing data in ratingsNorm with predictedRatingsNorm, then do the matrix factorization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdenseMatrix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpedictedRatingsNorm\u001b[0m \u001b[1;31m#this is probably not how this is done\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mInds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdid_rate\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdenseMatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mratingsNorm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#probably all this syntax is off\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pedictedRatingsNorm' is not defined"
     ]
    }
   ],
   "source": [
    "# fill in missing data in ratingsNorm with predictedRatingsNorm, then do the matrix factorization\n",
    "\n",
    "denseMatrix=np.empty_like(predictedRatingsNorm) \n",
    "denseMatrix[:]=predictedRatingsNorm\n",
    "Inds=np.where(did_rate==1)\n",
    "denseMatrix[Inds]=ratings_norm[Inds]\n",
    "\n",
    "# then do the collab filter\n",
    "svd = TruncatedSVD(n_components=3, n_iter=10, random_state=42)\n",
    "finalPredictedRatings = svd.fit_transform(denseMatrix)\n",
    "\n",
    "#how well does it do? test it on the last third of the data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
